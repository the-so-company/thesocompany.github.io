The So Company RFI Response
70--Ideas for Creating Mobile Applications for Veterans 
Solicitation Number: 36C10B18Q2598

###Company Information
####Company Name
Storij Inc DBA The So Company - SDVOSB, 8(a)
Company Address
2 Saint Nicholas Ave. #46
Brooklyn, NY 11211

####Point of contact name
Shaun So
Telephone number
929.888.5190
Email address
shaun@thesocompany.com

###Responses
	1.	How do you suggest VA incentivize the largest pool of contractors possible to develop popular mobile applications?  How could VA incentivize and reach contractors that have never worked in the Federal market? How could VA better partner with academia for such application development? What grant or other model would you recommend to engage such university-based start-up, innovation, or entrepreneurial communities?

In our answer we will focus specifically on small businesses and/or businesses for which Federal contracting is only a small part of the overall business. It is difficult to start up a Federal contracting practice in general, it is perhaps more difficult for commercially focused firms to start a Federal contracting practice that runs alongside their core business, and it is even more difficult for commercial technology firms. The business models can oftentimes be mutually exclusive and not match.

Also, even if a company does want to set up a new Federal contracting practice, will it get an opportunity to bid on work? In the aftermath of GAO’s Kingdomware decision and the “rule of two’s”, many more opportunities at VA will go to SDVOSB set-aside competitions. While there are many SDVOSB’s that can perform technology development work for VA, driving more competitions to SDVOSB set-asides does not support the goal of reaching contractors who have never worked in the Federal market.

We base our answers not only on our own experience, but also on our experience as consultants to a private technology company trying to deploy its commercial app into the US federal government, such as the Department of Defense. Because of the government client’s continued requests for customization and for the expenses required to deploy into the government’s technology stack, the contracting effort produced a significant drag on our client’s resources, which resulted in equally significant investor unease and also employee dissatisfaction. Over a yearlong engagement our client saw little revenue from its government effort, and the effort was eventually dropped. This is a not uncommon outcome for tech companies trying to work in the Federal government.

So what can VA do? First, utilize VA’s Center for Innovation (VACI). The So Company’s first work with VA was through VACI, and this initial effort has enabled us to provide support to VA continuously for the past three years, and also to expand into providing support for the Department of Health and Human Services. It worked for us and we believe that it can work for others. Second, VA could do more to be clear about how to on-ramp onto IDIQ’s, GSA schedules, or other GWACs, since this is where the bulk of money gets spent. As it stands now almost any company new to the Federal market is going to spend a year or two fussing around with FBO and trying to figure out the space, and in that time most companies are going to drop out. If VA could do more to capture new companies in this time period and help them get into a position where they can bid on task orders or where VA COs can purchase their services off of a schedule, this will increase VA’s ability to access companies that are new to the market. Otherwise, it will be the same old faces. Third it should encourage the “old faces” to team with mobile-app oriented, or mobile first, technology companies. By matching best-in-class technology talent from the private sector with companies that have successfully navigated set-aside contracts can incentive all participating companies accordingly.

Finally, VA needs to solicit bids for work in chunks of work that small business can actually afford to perform. Contracts in the $500k to $3MM range are much easier for small, start-up businesses to bid on. While it may be common for task orders to be issued in these amounts, or for VA or other Federal customers to purchase services of this size via GSA schedule or other GWACs, contracts open to free and open competition are less likely to be this size. This makes it harder for companies that are new to the market and who are not on schedules or IDIQs to find work. USDS and GSA’s 18f have been experimented with micro purchasing for short technology sprints with some success.

With regard to engaging academia, The So Company’s Partner Richard Wheeler, an Army Veteran who is an adjunct faculty member in Design at both UCLA and Art Center College of Design, adds the following:

While engaging with academia sounds great in theory, I think VA should be very clear about what it is trying to get out of that engagement to really get value for VA and at the end of the day for Veterans.

In my experience the academic cycle does not lend itself to effective technology development. You’ve got students for between 10 to 15 weeks, so somewhere between five to eight two-week sprints. But even that is not a real number because of the on-boarding time. So it’s really questionable whether students—even grad students—are going to be able to produce finished product in the available time.

If, however, VA looks at academic engagement as a way to generate ideas, that sounds like it might add more value. But even so it is really hard to get students to understand the issues and the problem sets well enough to generate good ideas. It’s hard enough to get professional developers and designers with no background in VA or with Veterans issues to do that, I’ve done Veteran-focused design projects in my classroom and I’ve seen other faculty members do the same and it’s just hard to cram in both learning about the subject matter and making something that isn’t completely simplistic. This isn’t unique to Veterans’ issues; you see the same thing when you try and engage other “social” issues—like homelessness, or food scarcity, or water use, or what have you—in the classroom.

One issue is that students of design and technology tend to have terrible practices of working with human subjects. That’s true of professionals in those disciplines as well. So VA would have to provide a lot of support in this area.

In engaging with academia the risk for blowback on VA is huge. It only takes one well meaning but misguided project to get back to Congress and then the Secretary is up in front of the Veterans Affairs Committees answering questions. Even relatively low-stakes grant amounts can generate a lot of heat. So VA has to ask whether the management of these efforts required to control risk is worth the benefits. Grants aren’t like contracts where you spec out what you want and it’s relatively clear what you are going to get and how you are going to get it. A lot can happen with that money on a campus.

	2.	Prior to investing resources to develop an application, how do you forecast the popularity of a mobile application?  How would you publicize the existence of your app to Veterans to get them to download/use it? What are the key analytics incorporated into mobile applications that could be used to gauge user experiences throughout the lifecycle of the application?
We believe that the underlying frame for the first part of this question seems to reflect a design approach both where a service or product has been designed without sufficient user involvement through requirements gathering and testing and where the project is built around waterfall methodologies, as opposed to continuous development methodologies. In the first case, The So Company believes that VA should continue with its pioneering work in applying human-centered design (HCD) principles to developing a unified user experience across VA products and services. 

With regard to the second part of the question about analytics, we find it problematic to generalize. The ultimate metric for VA is arguably whether Veterans get their benefits and whether they get them dealing with the least amount of bureaucratic friction. Typical analytics are not necessarily the best measures of these metrics, especially when one considers apps. Is number of downloads a “good” measure of an apps utility, or simply a measure that the app is addressed to a large population of Veterans, or addresses a population not adequately reached by previous efforts. Downloads will always be a numerator over some denominator representing some population. Similarly, is time spent in an app a “good” measure? We would say no: if users are in the app too much time this may be an indication that the underlying process to complete the task in the app are too long. These two examples also both speak to the problems of thinking about the utility of a technology in terms of “popularity”. By nature, VA is serving populations that may have very small numbers but very big problems. Consider the group of Veterans who may have been exposed to Agent Orange through service on C-123 aircraft during and after the end of the Vietnam conflict (https://www.vets.gov/disability-benefits/conditions/exposure-to-hazardous-materials/agent-orange/c-123/). In some cases, specifically Veterans who flew on these aircraft in the post-war era, Veterans may have to research the specific units and US Air Force Specialty Codes that were possibly affected by Agent Orange, and currently they must do this through a .pdf file, and one could easily see this as something that Veterans could research through some part of an app. But what would the success metrics of this app look like? The total number of Veterans affected during the post-war era is relatively small, so downloads and other typical measures are not going to make this notional app look very good. But is that what is important? 

We believe that appropriate analytics for any technology implementation must be developed through user experience research and testing that is conducted continuously throughout the technology’s development. There is no “one-size-fits-all” solution—and if you read differently elsewhere, someone is trying to sell you something.


	3.	How should VA select which mobile applications would be most popular among the many ideas that are received from industry?  What non-traditional acquisition methods do you recommend for procuring mobile apps? What new payment models would you recommend to ensure companies or development teams deliver results with constant iteration?
Industry will, and does, undoubtedly receive many unsolicited proposals from app companies. Some of these may even come from companies that have some background in serving Veterans—although many companies undoubtedly do not have this background. But “selecting” from these proposals—especially on the basis of “popularity”—strikes us as the wrong approach for VA and for Veterans.

VA can’t choose apps like a consumer selecting from an app store. VA has specific needs based on its special mission to serve Veterans, a mission which has no comparison in the private sector, even within private sector health care. Therefore while there may be aspects of apps or other technologies that apply to VA’s mission it is unlikely that VA can select apps and deploy them into VA’s technology stack without significant implementation costs.

Following from this, and from our earlier answers in question 1, we do not believe that focusing on developing non-traditional acquisition methods is the best approach to delivering technology products and services to Veterans.


	4.	Traditionally, the most popular and useful mobile applications for Veterans require integration and access with systems that contain VA sensitive (i.e. PII/PHI) information.  How would you simultaneously support both ease of access (i.e. easy logon procedures) and access to sensitive information? What do you consider the best mobile middleware products to securely link sensitive data from VA clinical and business systems with mobile applications?
As we will discuss elsewhere in our response, we believe in the approach to this question that VA has taken with Vets.gov, in other words, creating mobile first web pages that utilize centralized access systems to enable easy logon and access to sensitive information. This approach scales, reduces costs, and better addresses end-user security issues. Individual OS and device specific apps, each requiring their own logon and password, and each making their own requests to a user’s device for access to information on the device (e.g. contacts, location services) increases the user’s attack surface considerably. Inevitably, if VA follows an app-centric path, some app that VA produces, will compromise user security and release PII/PHI. Of course, one might argue that the risk of compromise exists with any technology system, and it does. We argue that the risk with individual apps is greater.

Also, with regard to user experience at sign-on, an app-based approach is clearly worse. Remembering multiple sets of logon credentials not only reduces security, it also frustrates users. This applies in general but also specifically to VA, where Veterans frequently complain about barriers to access and about frustrations around having to engage multiple touch-points to get their benefits. If we are looking at access in terms of creating a unified Veteran experience that is a Veteran¬-centric experience, answers clearly lie in reducing the amount of work that the Veteran has to do to log on to a system, get to the information that they need, and get the benefits that they have earned. This clear and compelling need should override any notional benefit that might be offered by possible functionality in a new app, especially when this functionality will come at the cost of increasing risk for the Veteran.

We believe that the current effort to use ID.me as an identity and Veteran-status validation management tool should be continued. ID.me already has integration with NARA to validate Veteran-status through DD-214 look-up—so why would VA reinvent the wheel? 


	5.	As mobile applications need to follow a pattern of continuous release and improvement, what do you recommend as an acquisition approach to ensuring dedicated teams necessary to provide such application support while minimizing conflicting priorities across an enterprise mobile application portfolio?
We believe in the original vision for Vets.gov as a platform from which individual applications can be supported, for a number of reasons:
•	Since Vets.gov was designed as a mobile first site, it is OS and device agnostic, which makes testing and deployment much easier than in cases of device specific apps. This is particularly true in the case of the Android platform, where multiple builds of the Android OS are deployed on many different manufacturers’ hardware. In our experience—both on projects for VA and for other Federal government clients—reducing the number of OSs and platforms against which new releases must be tested better supports continuous integration and improvement than a native-app approach. As we have pointed out in other answers, this approach scales more easily, reduces costs, and contributes to a more consistent user experience.
•	Acquisitions built around developing individual apps run the risk of focusing too much on the requirements for the particular app at the expense of enterprise-wide requirements, and the burden of addressing this risk ends up falling on the client (the group that is championing the app) and the CO that is supporting the client.

Take as an example some of The So Company’s core competencies, content-strategy and plain-language writing delivery. We’ve spent three years developing skills, processes, and relationships to work with constituencies within VA to accurately and plainly describe VA programs and services to Veterans. There are a lot of parts of this process that are not self-evident to those who don’t have our experience. So when “Team  New App” works with the CO supporting their effort to develop the RFP for their new app, neither the team nor the CO may adequately spec the amount of work required to create content for the app. They may not even know what they don’t know to ask about the level of effort in an RFI. And so when the competition happens and the award is made and the work starts, the level of effort may not be understood until the contractor starts running into problems. Maybe the contract is FFP and the risk is on the contractor and so the team and the CO don’t see this as a problem. But if that contractor goes out of business or decides that doing Federal government work isn’t worth the effort because of their experience, then going back to your sub-questions in question 1, how does this increase the pool of companies that VA can draw on to support technology development?


	6.	What would your recommendation be on establishing a unified approach to human centered and user experience design to maintain consistency across an enterprise portfolio of mobile applications? How would you address the needs for consistent Section 508 compliance across the portfolio? What do you recommend as an approach to ensure both Veteran or provide end-user prototyping with iterative testing and feedback for such industry developed mobile applications?
The answers to this question lie in VA’s need to continue to develop a broader HCD effort in order to build unified user experience. The challenges that VA faces in this overall effort apply to the subset of IT projects and the sub-subset of mobile IT projects.

One of VA’s largest challenges is in recruiting testers. VA has consistently failed to address the need to create a unified Veteran tester pool, as well as pools of other users like family members or care givers. This hampered and will continue to hamper technology development efforts, whether they be to create mobile products or to create other products. In our experience, each project is on it’s own to recruit testers, and is also on it’s own to develop and run testing procedures. 

Another challenge to successfully conducting design research within VA is obtaining institutional review board (IRB) approval. In terms of headcount and budget, VA is dominated by a healthcare organization: VHA. As with almost all healthcare organizations in the United States, human-subject research in VHA must obtain IRB approval. In our experience, VHA does not view design research any differently than other human-subject research in this regard, and so from VHA’s point-of-view, design research must obtain IRB approval from VA Central Institutional Review Board. Following from this, in our experience, lack of IRB approval for HCD research projects becomes a clear-cut justification for anyone at VA—VHA or otherwise—to say no to HCD projects. This has resulted in limited access to VA facilities and staff and has limited participation of some parts of VA in HCD efforts.

This lack of coordination and standardization clearly affects all users, but it impacts users who rely on 508 compliant systems even more. 508 compliance is a specialized field and expecting individual projects to address it on an ad hoc basis is not a recipe for success. We suggest that VA should test creating a centralized 508 compliance procedure—either hosted within OI&T or VEO—that will assess compliance for all new technology development. We further suggest that this effort first be applied to mobile “applications” developed within a centralized framework like Vets.gov. Doing so will enable VA to deploy more automated tools to assess compliance, and will enable VA to produce more consistent results across a wider range of products. We believe that this is the best approach to achieve 508 compliance—and greater usability in general—at scale and at low cost.

With regards to testing overall, as we noted above we similarly believe that continuing to follow an approach that creates mobile first web pages as opposed to OS- and device-specific mobile applications enables VA to deploy testing tools that work at scale and at a lower cost, especially when one considers the amount of testing required using modern iterative development techniques. Consider a case where VA creates a new testing procedure or invests in new testing tools to meet the needs of developing an app for a specific OS or a specific device only to see this OS or device go away. The same resources devoted to testing tools that can be used again and again are arguably a better investment for VA, for Veterans, and for the taxpayer.


	7.	Have you participated in prior VA mobile application development and what lessons learned do you have from that experience as it relates to the above? If you have not participated in VA mobile application development or if you have previously bid for such work and lost: what is your opinion of ongoing VA efforts, the efficiency and delivery of efforts to date, and how they can be improved?
We have participated in the development of Vets.gov from the beginning and we are clearly biased towards the approach and the benefits of this project. Some of the lessons that we have learned include:
•	Like every Federal department, VA has many internal constituencies, each fighting for control over budget. If you centralize the system in a platform like Vets.gov, you centralize the budget. If you allow individual subunits to develop their own apps, you fragment the budget. If you fragment the budget, you fail to realize economies of scale in purchasing and development, you reduce the opportunity to apply centralized quality-control measures around security, usability, and other critical elements of user-experience, and you open the organization to the development of frivolous technologies. Of course, centralization has it’s own risks. But we believe that in this case those risks are outweighed by the benefits, and that centralization and standardization is the way towards delivering a unified, Veteran-centric experience.
•	Since we started working on technology projects with VA in 2014, we have seen numerous “rogue” projects that have been stood up without proper oversight. This puts Veterans at risk. Everyone thinks that they can build a website or an app, because nowadays everyone pretty much can. But deploying code to a server is not delivering a service to Veterans. And if the work isn’t done to make sure a new technology is usable, and that it doesn’t expose and/or misuse PII/PHI, and that it doesn’t otherwise harm or put Veterans at risk, then the technology isn’t ready for deployment, period.

In conclusion, the development of mobile apps, as an industry best practice, has to serve the user as a function of mobility. Just building a mobile application for the sake of saying, “look at our app” has limited utility and function and has often become sinkholes of time and money. Many companies fail because they built a mobile application without a purpose. VA needs to understand what mobility problem they are trying to solve first, and then, through user-centered research, decide if a mobile application is the most effective means to solve that problem. Veterans are mobile, Veterans need support, and Veterans want engagement that delivers a quick and efficient service. Building mobile applications to support and engage is a monumental challenge that should carefully understood.
